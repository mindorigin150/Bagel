import os
import glob
from typing import Dict, Optional, Any
from torch import Value
from tqdm import tqdm
import re

from .judgement import BaseJudge, LocalJudge, APIModelJudge, JudgementError
from .inference_engine import VllmEngine, HfPipelineEngine

from .utils import (
    get_setting_from_id,
    initialize_basic_results_structure,
    update_accuracy_metrics,
    load_jsonl_data,
    save_json_results,
    print_basic_results
)

class BasicEvaluator:
    """ 
    Basic task evaluator for answer accuracy.
    """
    def __init__(self, judge: BaseJudge):
        self.judge = judge
        print(f"BasicEvaluator initialized with judge: {self.judge.__class__.__name__}")
    
    def evaluate(self, jsonl_path: str, output_path: Optional[str] = None) -> Dict:
        data = load_jsonl_data(jsonl_path)
        
        results = initialize_basic_results_structure()
        results['unfiltered_total'] = len(data)
        error_cases = {'extraction_error': [], 'judgement_error': []}
        items_to_judge = []
        original_indices = []
        # iterate over each data
        pattern = r"\[Question\]\n(.*)"
        for i, item in enumerate(data):
            if item.get('gt_answer'):
                input_prompt = item.get('input_prompt', '')
                question = re.search(pattern, input_prompt, re.DOTALL)
                model_answer = item.get('answer', '')
                gt_answer = item.get('gt_answer', '')
                # pack question, model_answer and gt_answer to items list
                items_to_judge.append((question, model_answer, gt_answer))
                # record corresponding indexes
                original_indices.append(i)
        print(f"‚öñÔ∏è  Processing {len(items_to_judge)} items...")

        # traverse results stream generated by batch_judge
        judged_count = 0
        try:
            # judgement_stream is a generator
            judgement_stream = self.judge.batch_judge(items_to_judge)
            
            for original_idx_in_subset, judgement in judgement_stream:
                # indexes for mapping back to original data list
                original_data_index = original_indices[original_idx_in_subset]
                item = data[original_data_index]
                # record results
                item['judgement_is_correct'] = judgement['is_correct']
                item['judgement_reason'] = judgement['reason']
                item['judge_model'] = self.judge.model
                judged_count += 1
        
        except JudgementError as e:
            print(f"\n‚ùå FATAL: Batch judgement failed. Error: {e}")
            error_cases['judgement_error'].append({'id': 'BATCH_FAILED', 'error': str(e)})
            return {'results': results, 'error_cases': error_cases, 'processed_data': data}
        print(f"‚úÖ Integration complete. {judged_count} items were judged.")
        
        # ---- Integrate and calculate ----
        filtered_total = 0
        filtered_correct = 0
        # traverse data to compute final result
        for item in data:
            if 'judgement_is_correct' not in item:
                # skip unjudged items
                continue
            setting = get_setting_from_id(item.get('id', ''))
            results['settings'][setting]['total'] += 1
            include_in_overall = results['settings'][setting].get('include_in_overall', True)
            if include_in_overall:
                filtered_total += 1
            
            if item['judgement_is_correct']:
                results['settings'][setting]['gen_cogmap_correct'] += 1
                if include_in_overall:
                    filtered_correct += 1
        results['total'] = filtered_total
        results['gen_cogmap_correct'] = filtered_correct
        results = update_accuracy_metrics(results)
        
        final_results = {'results': results, 'error_cases': error_cases, 'processed_data': data}
        
        print_basic_results(results)
        if output_path:
            save_json_results(final_results, output_path)
        
        return final_results


def create_judge(judge_config: Optional[Dict[str, Any]]) -> Optional[BaseJudge]:
    """Factory function to create a judge instance based on config."""
    if not judge_config:
        raise ValueError("Must provide configuration for judge!")
    
    judge_type = judge_config.get("judge_type")
    # API judge
    if judge_type == "api":
        api_config = judge_config.get("api_config")
        if not api_config:
            raise ValueError("Must provide configuration for API Model Judge!")

        return APIModelJudge(**api_config)
    # local model judge
    elif judge_type == "local":
        engine_type = judge_config.get("engine", "vllm")
        print(f"üîß Creating '{engine_type}' engine for local judge...")
        
        # if engine type is vllm
        if engine_type == "vllm":
            vllm_config = judge_config.get("vllm_config")
            if not vllm_config: raise ValueError("vllm config must be provided for local judge.")
            engine = VllmEngine(vllm_config)
        # else if engine type is huggingface transformer
        elif engine_type == "hf":
            model = judge_config.get("model")
            engine = HfPipelineEngine(model)
        else: raise ValueError(f"Unknown engine type: {engine_type}")

        return LocalJudge(engine)
    else:
        raise ValueError(f"Unsupported judge type: {judge_type}")


def batch_evaluate(directory: str, output_dir: Optional[str] = None, 
                   judge_config: Optional[Dict[str, Any]] = None) -> bool:
    """
    Efficiently evaluates all .jsonl files in a directory using BasicEvaluator.
    The judge model is loaded only ONCE.
    """
    jsonl_files = sorted(glob.glob(os.path.join(directory, '**', '*.jsonl'), recursive=True))
    if not jsonl_files:
        print(f"‚ö†Ô∏è No .jsonl files found in {directory}. Nothing to do.")
        return True

    try:
        judge = create_judge(judge_config)
    except Exception as e:
        print(f"‚ùå FATAL: Failed to create the judge. Aborting. Error: {e}")
        return False

    print(f"üîç Found {len(jsonl_files)} files to evaluate.")
    
    # Create the evaluator once with the pre-loaded judge.
    evaluator = BasicEvaluator(judge=judge)
    
    for i, file_path in enumerate(jsonl_files):
        print(f"\n--- [{i+1}/{len(jsonl_files)}] Processing: {os.path.basename(file_path)} ---")
        output_path = None
        if output_dir:
            relative_path = os.path.relpath(file_path, directory)
            base, _ = os.path.splitext(relative_path)
            output_path = os.path.join(output_dir, f"{base}_eval_results.json")
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        try:
            evaluator.evaluate(file_path, output_path)
            print(f"‚úÖ Successfully evaluated {os.path.basename(file_path)}")
        except Exception as e:
            print(f"‚ùå FAILED to evaluate {os.path.basename(file_path)}. Error: {e}")

    print("\nüéâ All files processed successfully!")
    return True
