# run_evaluation.py (ç²¾ç®€å)
import sys
import os
import argparse
from typing import Dict, Any, Optional

current_dir = os.path.dirname(__file__)
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)



def main():
    from evaluation.evaluator import batch_evaluate, BasicEvaluator, create_judge
    parser = argparse.ArgumentParser(
        description='Evaluation Framework',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Rule-based evaluation on a single file
  python run_evaluation.py -i responses.jsonl
  
  # Batch evaluation with a local judge
  python run_evaluation.py -b results/ -o analysis/ --judge-type local --judge-model-path /path/to/model
        """
    )
    
    # --- Main action arguments ---
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--input', '-i', type=str, help='Input JSONL file path')
    group.add_argument('--batch_dir', '-b', type=str, help='Directory for batch evaluation')
    
    # --- Output arguments ---
    parser.add_argument('--output', '-o', type=str, help='Output file path for single evaluation')
    parser.add_argument('--output_dir', type=str, help='Output directory for batch evaluation')
    
    # --- Simplified Options ---
    parser.add_argument('--quiet', '-q', action='store_true', help='Quiet mode')

    # --- Judgement Options ---
    judge_group = parser.add_argument_group('Judgement Options', 'Configure a model-based judge.')
    judge_group.add_argument('--judge-type', type=str, choices=['api', 'local'],
                             help='Type of judge model. If not set, uses rule-based matching.')
    judge_group.add_argument('--judge-model-path', type=str,
                             help='Path to local judge model (for --judge-type local).')
    judge_group.add_argument('--tensor-parallel-size', type=int, help='Tensor parallel size for vllm.')
    judge_group.add_argument('--judge-api-endpoint', type=str,
                             help='API endpoint for judge model (for --judge-type api).')
    judge_group.add_argument('--judge-api-key', type=str, help='API key (optional).')
    judge_group.add_argument('--engine', type=str, choices=['vllm', 'hf'], default='vllm',
                             help='Inference engine for local judge (default: vllm).')
    judge_group.add_argument('--judge-model-name', type=str, help='Model name for API judge')
    
    args = parser.parse_args()

    judge_config: Optional[Dict[str, Any]] = None
    if args.judge_type:
        if args.judge_type == 'local' and not args.judge_model_path:
            parser.error("--judge-model-path is required for --judge-type local")
        
        judge_config = {
            "judge_type": args.judge_type, "model_path": args.judge_model_path, 'tensor_parallel_size': args.tensor_parallel_size,
            "api_endpoint": args.judge_api_endpoint, "api_key": args.judge_api_key,
            "engine": args.engine, "model_name": args.judge_model_name
        }
        if not args.quiet: print(f"âš–ï¸  Using '{args.judge_type}' judge.")
    else:
        if not args.quiet: print("âš–ï¸  Using default rule-based matching.")

    try:
        if args.batch_dir:
            success = batch_evaluate(args.batch_dir, args.output_dir, judge_config=judge_config)
            if not success:
                sys.exit(1)
            
        elif args.input:
            if not os.path.exists(args.input):
                print(f"âŒ Error: Input file '{args.input}' not found")
                sys.exit(1)

            # For single file evaluation, we create the judge and evaluator here.
            judge = create_judge(judge_config)
            evaluator = BasicEvaluator(judge=judge)
            evaluator.evaluate(args.input, args.output)
            if args.output:
                print(f"ğŸ“ Results saved to {args.output}")

    except Exception as e:
        print(f"âŒ An unhandled error occurred: {e}", file=sys.stderr)
        if not args.quiet:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # ------------------- æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œ -------------------
    import multiprocessing
    
    # åœ¨Linuxä¸Šï¼Œé»˜è®¤æ˜¯ 'fork'ï¼Œè¿™åœ¨å¤šGPUçš„CUDAç¯å¢ƒä¸­æ˜¯è‡´å‘½çš„ã€‚
    # æˆ‘ä»¬å¿…é¡»åœ¨ä»»ä½•CUDA/PyTorch/VLLMæ“ä½œä¹‹å‰ï¼Œ
    # å¼ºåˆ¶å°†è¿›ç¨‹å¯åŠ¨æ–¹æ³•è®¾ç½®ä¸º 'spawn'ã€‚
    # "force=True" æ˜¯ä¸ºäº†ç¡®ä¿å³ä½¿å®ƒå·²ç»è¢«è®¾ç½®è¿‡ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½è¦†ç›–å®ƒã€‚
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        # å¦‚æœå¯åŠ¨æ–¹æ³•å·²ç»è¢«è®¾ç½®ï¼Œå¯èƒ½ä¼šæŠ›å‡º RuntimeErrorã€‚
        # æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°å¿½ç•¥å®ƒï¼Œå› ä¸ºå®ƒå¯èƒ½å·²ç»è¢«æ­£ç¡®è®¾ç½®äº†ã€‚
        pass
    # ----------------------------------------------------
    main()